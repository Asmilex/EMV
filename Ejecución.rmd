---
title: "Análisis exploratorio (ejecuciones)"
date: "`r format(Sys.time(), '%d %B, %Y')`"
author:
- Andrés Millán
- Paula Villanueva
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
    theme: united
    highlight: tango
  pdf_document:
    toc: true
    toc_depth: 3
    number_sections: true
    theme: united
    highlight: tango
---

# Análisis exploratorio univariante

Se ha elegido el fichero de datos `DB_2.sav`. La información del dataset es la siguiente:

> Un grupo constituido por 13 empresas se ha clasificado según las puntuaciones obtenidas en 8 indicadores económicos:
>
> - X1: Indicador de volumen de facturación.
> - X2: Indicador de nivel de nueva contratación.
> - X3: Indicador del total de clientes.
> - X4: Indicador de beneficios de la empresa .
> - X5: Indicador de retribución salarial de los empleados.
> - X6: Indicador de organización empresarial dentro de la empresa.
> - X7: Indicador de relaciones con otras empresas.
> - X8: Indicador de nivel de equipamiento (ordenadores, maquinaria, etc...).

Comenzamos cargando la base de datos con el siguiente comando y mostramos los datos que contiene.

```{R}
library(foreign)
datos <- read.spss("./datasets/DB_2.sav", to.data.frame = TRUE, reencode = "utf-8")
datos
```

Según el problema, hay 13 empresas pero observamos que hay 14 filas. Esto nos hace sospechar que hay una fila errónea.



## Recodificaciones

No es necesario recodificar o agrupar los datos, pues como observamos en el archivo de datos, tenemos que todas las variables son numéricas.


## Valores perdidos

Observamos que la variable `x7` tiene un valor perdido, así como la variable `x8`. Por lo que el porcentaje de valores perdidos de las variables `x1`,...,`x6` es del 0%, y de las variables `x7` y `x8` es del 0.071428%.

Veamos los estadísticos descriptivos básicos con el siguiente comando.

```{R}
summary(datos)
```

Además, nos llama la atención que la media de la variable `x1` obtenga un valor demasiado elevado. Esto se debe a que en la fila 14 hay un valor muy grande y hace que se alteren las medidas que resumen a las variables. Asimismo, como habíamos mencionado antes, la fila 14 es la que tiene los valores perdidos y, como todo indica que las primeras 13 filas corresponden a las 13 primeras empresas, procedemos a eliminar esta última fila con el siguiente comando.

```{R}
datos <- datos[-14, ]
colSums(is.na(datos))
```

Como no tenemos más valores perdidos en el dataset, no necesitamos analizar el patrón aleatorio de éstos ni cómo imputarlos.

## Análisis descriptivo numérico clásico

Hasta ahora, el resumen de los datos que tenemos es el siguiente.

```{R}
summary(datos)
```

Con el siguiente comando crearemos un boxplot para ver cómo es la distribución de las variables.

```{R}
boxplot (
  datos,
  main = "Análisis exploratorio de datos",
  xlab = "Indicadores",
  ylab = "Valor",
  col = c(1 : 15)
)
```

Además, para obtener el coeficiente de simetría utilizaremos el comando `skewness`.

https://www.programmingr.com/statistics/skewness/

```{r}
library(moments)
skewness(datos)
```

## Valores extremos

Utilizamos la función [`check_outliers`](https://www.rdocumentation.org/packages/performance/versions/0.8.0/topics/check_outliers) para comprobar si hay outliers:

```{R}
library("performance")
check_outliers(datos, method = "mahalanobis")
```

Aunque en el boxplot anterior sí que se indican posibles valores outliers, utilizando el método de mahalanobis no sale nada. Por tanto, los dejaremos en el conjunto de datos.

## Supuesto de normalidad

Comprobamos si los datos están normalizados (es decir, media 0 y varianza 1). Para ello, calculamos la media y la desviación típica de cada variable con los siguientes comandos.

```{r}
round(colMeans(datos), 5)
```

```{R}
round(apply(datos, 2, sd), 5)
```

No lo están, así que los normalizamos:

```{r}
datos_normalizados <- scale(datos)
datos_normalizados
```

Y ahora, podemos comprobar que lo están:

```{r}
round(colMeans(datos_normalizados), 5)
```

```{R}
round(apply(datos_normalizados, 2, sd), 5)
```

Además, podemos hacer uso del gráfico qqplot para mostrar si hay normalidad:

```{r}
par(mar = c(1, 1, 1, 1))
par(mfrow = c(2, 4))

invisible(apply(datos_normalizados, 2, function(x) {
  qqnorm(x, main = NULL)
  abline(a = 0, b = 1, col = "#00b894")
}))
```

(Comentar esto)


# Análisis exploratorio multivariante

```{r}
datos_pca <- datos_normalizados
```

## Correlación entre variables (test de Bartlett)

Comprobamos si existe correlación entre las variables usando el test de Bartlett.

```{r}
library("psych")
cortest.bartlett(cor(datos_pca), nrow(datos_pca))
```

Observamos que obtenemos un `p-value` prácticamente nulo, por lo que esto indica que las variables están correladas. De esta forma, procederemos a realizar un Análisis de Componentes Principales (ACP).


```{r}
round(cor(datos_pca), 5)
```


## Tratar outliers si no se ha hecho antes

```{R}
boxplot (
  datos_pca,
  main = "Análisis exploratorio de datos",
  xlab = "Indicadores",
  ylab = "Valor",
  col = c(1 : 15)
)
```

```{R}
check_outliers(datos_pca, method = "mahalanobis")
```

Decidir si los eliminamos finalmente.

```{r}
outlier <- function(data, na_rm = T) {
        H <- 1.5 * IQR(data)
        data[data < quantile(data, 0.25, na.rm = T) - H] <- NA
        data[data > quantile(data, 0.75, na.rm = T) + H] <- NA
        data[is.na(data)] <- mean(data, na.rm = T)
        data
}

#datos_pca <- apply(datos_pca, 2, outlier)
```

```{R}
datos_pca
```

## Análisis de componentes principales

Realizamos el ACP con la siguiente función.

```{R}
PCA <- prcomp(datos_pca, scale = T, center = T)
```

El campo `rotation` del objeto `PCA` es una matriz cuyas columnas son los coeficientes de las componentes principales. Esto es, podemos ver el peso de cada variable en la correspondiente componente principal.

```{R}
PCA$rotation
```

Además, con el campo `sdev` del objeto `PCA` y con la función `summary` aplicada al objeto, obtenemos las desviaciones típicas de cada componente principal y la proporción de varianza explicada y acumulada.

```{R}
PCA$sdev
summary(PCA)
```

Con el siguiente comando hacemos un análisis gráfico de la varianza explicada.

```{R}
library(ggplot2)
varianza_explicada <- PCA$sdev^2 / sum(PCA$sdev^2)
ggplot(
  data = data.frame(varianza_explicada, pc = 1:8),
  aes(x = pc, y = varianza_explicada, fill = varianza_explicada)
) + geom_col(width = 0.3) +
    scale_y_continuous(limits = c(0,0.6)) +
    theme_minimal() +
    labs(x = "Componente principal", y= "Proporcion de varianza explicada")
```

Con el siguiente comando hacemos un análisis gráfico de la varianza explicada.

```{R}
varianza_acum <- cumsum(varianza_explicada)
ggplot(
  data = data.frame(varianza_acum, pc = 1:8),
  aes(x = pc, y = varianza_acum ,fill=varianza_acum )
) + geom_col(width = 0.5) +
    scale_y_continuous(limits = c(0,1)) +
    theme_minimal() +
    labs(x = "Componente principal", y = "Proporcion varianza acumulada")
```


### Reducción de dimensión mediante variables observables

En esta sección seleccionaremos el número de componentes principales óptimo.

Para ello, se promedian las varianzas explicadas por las componentes principales y se seleccionan aquellas cuya proporción de varianza explicada supera la media.

Calculamos la varianza con el siguiente comando.

```{R}
PCA$sdev^2
```

Calculamos el promedio de la varianza con la siguiente función.

```{R}
mean(PCA$sdev^2)
```

Por lo que nos quedaremos con las **tres primeras componentes principales**, pues son las que su proporción de varianza explicada supera la media, que es 1.

Además, para afianzar esta decisión, podemos hacer uso del método del codo, como se indica a continuación.

```{r}
ggplot(
  data = data.frame(varianza_acum, pc = 1:8),
  aes(x = pc, y = varianza_acum)
) + geom_line(size = 1, colour = "#00b894") +
    geom_point(size = 2.5, colour = "#2c3e50") +
    scale_y_continuous(limits = c(0, 1)) +
    theme_minimal() +
    labs(x = "Componente principal", y = "Proporcion varianza acumulada")
```

### Visualización de las contribuciones a las dimensiones principales

En este apartado veremos la representación conjunta de variables y observaciones que relaciona visualmente las posibles relaciones entre las observaciones, las contribuciones de los individuos a las varianzas de las componentes y el peso de las variables en cada componentes principal.

Variables y observaciones en la primera y segunda componente principal:

```{r}
library("factoextra")
fviz_pca(
  PCA,
  axes = c(1, 2),
  alpha.ind = "contrib",
  col.var = "cos2", col.ind = "#2c3e50",
  gradient.cols = c("#ddd839", "#f7a438", "#e9473b"),
  repel = TRUE,
  legend.title = "Distancia"
) + theme_bw()

```

Variables y observaciones en la primera y tercera componente principal:

```{r}
fviz_pca(
  PCA,
  axes = c(1, 3),
  alpha.ind = "contrib",
  col.var = "cos2",
  col.ind = "#2c3e50",
  gradient.cols = c("#ddd839", "#f7a438", "#e9473b"),
  repel = TRUE,
  legend.title = "Distancia"
) + theme_bw()
```

Variables y observaciones en la segunda y tercera componente principal:

```{R}
fviz_pca(
  PCA,
  axes = c(2, 3),
  alpha.ind = "contrib",
  col.var = "cos2",
  col.ind = "#2c3e50",
  gradient.cols = c("#ddd839", "#f7a438", "#e9473b"),
  repel = TRUE,
  legend.title = "Distancia"
  ) + theme_bw()
```


## Análisis factorial (variables latentes)

Ya hemos visto que las variables están correladas, luego tiene sentido el AF.

Obtenemos una representación visual de las correlaciones. Calculamos la matriz de correlaciones policórica.

```{R}
library("polycor")
library("ggcorrplot")
datos_af <- datos_normalizados
poly_cor <- hetcor(datos_af)$correlations
ggcorrplot(poly_cor, type = "lower", hc.order = T)
```

Ahora, compararemos las salidas con el método del factor principal y con el de máxima verosimilitud.

```{R}
modelo1 <- fa(
  poly_cor,
  nfactors = 3,
  rotate = "none",
  fm = "mle"
) # modelo máxima verosimilitud
```

```{R}
modelo2 <- fa(
  poly_cor,
  nfactors = 3,
  rotate = "none",
  fm = "minres"
) # modelo mínimo residuo
```

Comparamos las comunalidades.

```{R}
sort(modelo1$communality,decreasing = T) -> c1
sort(modelo2$communality,decreasing = T) -> c2
head(cbind(c1,c2))
```

Comparamos las unicidades, es decir, la proporción de varianza que no ha sido explicada por el factor (1-comunalidad).

```{R}
sort(modelo1$uniquenesses, decreasing = T) -> u1
sort(modelo2$uniquenesses, decreasing = T) -> u2
head(cbind(u1,u2))
```

Determinamos el número óptimo de factores.

```{R}
scree(poly_cor)
fa.parallel(poly_cor, n.obs = 13, fa = "fa", fm = "mle")
```

Estimamos el modelo factorial con 3 factores implementando una rotación tipo varimax para buscar una interpretación más simple.

```{R}
modelo_varimax <- fa(poly_cor, nfactors = 3, rotate = "varimax", fa = "mle")
```
Mostramos la matriz de pesos factorial rotada.

```{R}
print(modelo_varimax$loadings, cut = 0)
fa.diagram(modelo_varimax)
```

En este diagrama, el primer factor está asociado con los items `x6`, `x7` y `x8`. El segundo factor está asociado con los items `x1`, `x2` y `x3`. El tercer factor está asociado con los items `x4` y `x5`.

Veamos con el test de hipótesis que contrasta si el número de factores es suficiente.

```{R}
library(stats)
factanal(datos_af, factors = 3, rotation="none")
```


## Análisis de la normalidad multivariante

El paquete MVN contiene funciones que permiten realizar los test que se utilizan habitualmente para contrastar la normalidad multivariante. Además, podemos analizar los outliers con funciones de este paquete.

```{r}
library(MVN)
hz_test <- mvn(data = datos_pca, mvnTest = "hz", multivariateOutlierMethod = "quan")
```

Se detectan 2 outliers en las observaciones 3 y 7. Sin embargo ninguno de los dos test realizados a continuación encuentran evidencias al 5% de significación de falta de normalidad multivariante.

A continuación realizamos el test de Royston.

```{R}
royston_test <- mvn(data = datos_pca, mvnTest = "royston", multivariatePlot = "qq")
```

```{r}
royston_test$multivariateNormality
```

El test de Henze-Zirkler es el siguiente.

```{R}
hz_test <- mvn(data = datos[, -1], mvnTest = "hz")
hz_test$multivariateNormality
```

Ambos nos dicen que siguen una normal multivariante.

## Clasificación

```{r}
# install.packages("tidyverse")
# install.packages("cluster")
# install.packages("factoextra")

# Cargamos los paquetes indicados
library(tidyverse)
library(cluster)
library(factoextra)
```

Matriz de distancias:

```{r}
distancias <- get_dist(datos_normalizados)

fviz_dist(
  distancias,
  gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07")
)
```

Aplicamos clustering con K-Medias:

```{r}
k2 <- kmeans(datos_normalizados, centers = 2, nstart = 25)
k3 <- kmeans(datos_normalizados, centers = 3, nstart = 25)
k4 <- kmeans(datos_normalizados, centers = 4, nstart = 25)
k5 <- kmeans(datos_normalizados, centers = 5, nstart = 25)

# plots to compare
p1 <- fviz_cluster(k2, geom = "point", data = datos_normalizados) + ggtitle("k = 2")
p2 <- fviz_cluster(k3, geom = "point", data = datos_normalizados) + ggtitle("k = 3")
p3 <- fviz_cluster(k4, geom = "point", data = datos_normalizados) + ggtitle("k = 4")
p4 <- fviz_cluster(k5, geom = "point", data = datos_normalizados) + ggtitle("k = 5")


library(gridExtra)
grid.arrange(p1, p2, p3, p4, nrow = 2)
```

Buscando el número óptimo de clusters:

```{r}
fviz_nbclust(datos_normalizados, kmeans, method = "wss")
```

```{r}
fviz_nbclust(datos_normalizados, kmeans, method = "silhouette")
```

```{r}
gap_stat <- clusGap(datos_normalizados, FUN = kmeans, nstart = 25, K.max = 10, B = 50)
fviz_gap_stat(gap_stat)
```

Notamos que hay inconsistencias en la búsqueda del número óptimo de clusters. Es altamente probable que necesitemos más datos.

Probamos con 4:

```{r}
print(k4)
```

```{r}
p3
```

## Validación básica; matriz de confusión

```{R, purl=FALSE, echo=FALSE, results=FALSE, message=FALSE, fig.show='hide', warning=FALSE}
knitr::purl("./Ejecución.rmd")
```