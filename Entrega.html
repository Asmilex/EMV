<h1 id="abstract">Abstract</h1>
<p>Se ha realizado un análisis exploratorio de un conjunto de datos. Este dataset recoge 8 indicadores económicos de 13 empresas. Tras estudiar la información recogida en el conjunto, tratando posibles valores perdidos y outliers, se han aplicado dos tipos de técnicas:</p>
<ul>
<li><strong>Análisis univariante numérico y gráfico</strong>. En él, se ha elaborado un análisis descriptivo numérico clásico y un análisis de supuesto de normalidad.</li>
<li><strong>Técnicas multivariantes</strong>: se ha estudiado la correlación entre variables, la reducción de la dimensión mediante variables observables y latentes. Además, se ha estudiado la normalidad multivariante de los datos.</li>
</ul>
<p>Finalmente, se ha construido un clasificador basado en clustering no jerárquico con el fin de estudiar cómo se agrupan las diferentes empresas. Descubrimos que existen 4 grupos diferentes.</p>
<h1 id="introducción">Introducción</h1>
<p>Se ha realizado el análisis exploratorio de los datos contenidos en la base de datos <code>DB_2</code>. Esta base de datos contiene un grupo constituido por 13 empresas que se ha clasificado según las puntuaciones obtenidas en 8 indicadores económicos.</p>
<p>Primero, se ha limpiado el dataset de cualquier anomalía posible. Hemos encontrado una instancia probablemente errónea, que contenía valores perdidos e indicadores sin ningún sentido. A continuación, se ha realizado un análisis descriptivo numérico clásico, esto es, se han obtenido las medidas de tendencia central, los cuartiles, el coeficiente de simetría, la dispersión, etc. Además, se han estudiado posibles outliers. Se ha comprobado también la normalidad de las variables individualmente mediante gráficos de normalidad.</p>
<p>Una vez preparado el conjunto inicial, procedemos a realizar el análisis exploratorio multivariante. Se comprobó la correlación entre las variables mediante un test de Bartlett. A continuación, se realizó un estudio de la posibilidad de reducción de la dimensión mediante variables observables, en cuyo caso se ha elegido el número óptimo de componentes principales usando distintas técnicas gráficas, y mediante variables latentes, en cuyo caso se ha elegido el número óptimo de factores a considerar. Lo siguiente fue analizar la normalidad multivariante de los datos con los tests con el paquete MVN.</p>
<p>Finalmente, para completar nuestro objetivo, se ha realizado un análisis cluster, es decir, un agrupamiento de los objetos formando clusters de objetos con un alto grado de homogeneidad interna y heterogeneidad. En concreto, se ha utilizado el método de las k medias, un método no jerárquico.</p>
<h1 id="materiales-y-métodos">Materiales y métodos</h1>
<h2 id="materiales">Materiales</h2>
<p>La base de datos elegida contiene un grupo constituido por 13 empresas que se ha clasificado según las puntuaciones obtenidas en 8 indicadores económicos:</p>
<ul>
<li>X1: Indicador de volumen de facturación.</li>
<li>X2: Indicador de nivel de nueva contratación.</li>
<li>X3: Indicador del total de clientes.</li>
<li>X4: Indicador de beneficios de la empresa .</li>
<li>X5: Indicador de retribución salarial de los empleados.</li>
<li>X6: Indicador de organización empresarial dentro de la empresa.</li>
<li>X7: Indicador de relaciones con otras empresas.</li>
<li>X8: Indicador de nivel de equipamiento (ordenadores, maquinaria, etc…).</li>
</ul>
<p>A continuación se muestra una tabla con los estadísticos descriptivos básicos.</p>
<pre><code>##        x1               x2                x3               x4         
##  Min.   : 0.128   Min.   : 0.9444   Min.   : 2.167   Min.   : 0.0299  
##  1st Qu.: 2.476   1st Qu.: 8.6931   1st Qu.: 8.667   1st Qu.: 4.0673  
##  Median : 7.584   Median :14.8487   Median :15.167   Median : 5.4044  
##  Mean   : 6.957   Mean   :14.9138   Mean   :15.167   Mean   : 5.3976  
##  3rd Qu.:10.734   3rd Qu.:21.7262   3rd Qu.:21.667   3rd Qu.: 6.5147  
##  Max.   :14.364   Max.   :25.4261   Max.   :28.167   Max.   :11.3959  
##        x5               x6               x7               x8        
##  Min.   : 2.557   Min.   : 6.135   Min.   : 1.064   Min.   : 3.949  
##  1st Qu.: 3.525   1st Qu.:10.170   1st Qu.: 3.982   1st Qu.: 6.833  
##  Median : 5.336   Median :11.374   Median : 5.584   Median : 8.103  
##  Mean   : 5.107   Mean   :21.006   Mean   : 6.598   Mean   :13.074  
##  3rd Qu.: 5.874   3rd Qu.:31.586   3rd Qu.:10.160   3rd Qu.:20.154  
##  Max.   :10.037   Max.   :43.278   Max.   :12.374   Max.   :26.571</code></pre>
<pre><code>## [1] &quot;Desviaciones estándar:&quot;</code></pre>
<pre><code>##        x1        x2        x3        x4        x5        x6        x7        x8 
##  4.545099  8.162600  8.437954  2.823818  1.991911 13.779784  4.030662  8.249730</code></pre>
<p>En la siguiente gráfica se muestran los diagramas de cajas de las variables.</p>
<div class="figure">
<img src="figure/fig.-1.png" alt="" />
<p class="caption">Diagrama de cajas</p>
</div>
<h2 id="métodos-estadísticos">Métodos estadísticos</h2>
<p>En este apartado se indican las distintas técnicas estadísticas que se han utilizado.</p>
<p>Primero, se ha realizado un análisis numérico y gráfico de cada variable. De esta forma, mediante en visionado de la estructura del archivo de datos, se han estudiado las posibles recodificaciones y valores perdidos. También se ha realizado un análisis descriptivo numérico clásico, esto es, usando las funciones <code>summary</code>, <code>boxplot</code> y <code>skewness</code> hemos obtenido las medidas de tendencia central, dispersión, cuartiles, simetría, etc. Por otra parte, con la función <code>check_outliers</code> hemos detectado los posibles outliers mediante el método de mahalanobis. Para comprobar el supuesto de normalidad, hemos utilizado <code>colMeans</code> y para normalizar los datos hemos usado <code>scale</code>. De esta forma, podemos visualizar la normalidad con <code>qqplot</code>.</p>
<p>Con respecto al análisis exploratorio multivariante, se ha utilizado el test de Bartlett, <code>cortest.bartlett</code>, para estudiar la correlación entre las variables. En cuanto al Análisis de Componentes Principales, éste se ha realizado con <code>prcomp</code> y se han utilizado técnicas gráficas, tales como <code>ggplot</code> y <code>fviz_pca</code>. Sobre el AF, se han utilizado otras técnicas gráficas, como <code>ggcorrplot</code>, <code>scree</code>, <code>parallel</code> y <code>diagram</code>, y <code>factanal</code> para realizar el test de hipótesis que constrasta si el número de factores es suficiente. Para realizar el análisis de la normalidad multivariante, se ha utilizado el paquete <code>MVN</code>. Específicamente, hemos usado dos tests diferentes: el de Henze-Zirkler y el de Royston.</p>
<p>Finalmente, para realizar el agrupamiento de los objetos, se ha utilizado la técnica <code>kmeans</code>, variando el número de clusters con el fin de comprobar cómo se agrupan las empresas.</p>
<h1 id="resultados">Resultados</h1>
<p>En este apartado se mostrarán los resultados obtenidos aplicando las técnicas mencionadas anteriormente.</p>
<h2 id="análisis-exploratorio-univariante">Análisis exploratorio univariante</h2>
<p>Para estudiar nuestro conjunto de datos, podemos obtener el coeficiente de simetría de la distribución estadística.</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">skewness</span>(datos)</span></code></pre></div>
<pre><code>##            x1            x2            x3            x4            x5            x6 
## -6.571128e-02 -2.178143e-01  2.441493e-16  8.611432e-02  9.539367e-01  4.214678e-01 
##            x7            x8 
##  1.014152e-01  4.874442e-01</code></pre>
<p>Antes de proceder con el PCA y el AF, es necesario tratar los outliers. Este fue un punto de discusión importante, pues como vimos en la figura 1, se muestran tres outliers en las variables x4, x5. Sin embargo, utilizando el método de Mahalanobis, encontramos que no se detecta ninguno:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="fu">check_outliers</span>(datos, <span class="at">method =</span> <span class="st">&quot;mahalanobis&quot;</span>)</span></code></pre></div>
<pre><code>## OK: No outliers detected.</code></pre>
<p>Tras normalizar el conjunto de datos, estudiamos cómo se distribuían las variables, produciendo el siguiente resultado:</p>
<div class="figure">
<img src="figure/unnamed-chunk-6-1.png" alt="" />
<p class="caption">Aproximación a normales univariantes</p>
</div>
<h2 id="análisis-explotatorio-multivariante">Análisis explotatorio multivariante</h2>
<h3 id="análisis-de-componentes-principales">Análisis de componentes principales</h3>
<p>Se ha comprobado que existe correlación entre las variables usando el test de Bartlett, pues obteníamos un <code>p-valor</code> prácticamente nulo. Esto indica que las variables están correladas, luego procederemos a realizar un Análisis de Componentes Principales (ACP).</p>
<p>Se han obtenido las desviaciones típicas de cada componente principal y la proporción de varianza explicada y acumulada. Podemos observar en la siguiente imagen un análisis gráfico de la varianza explicada.</p>
<div class="figure">
<img src="figure/unnamed-chunk-7-1.png" alt="" />
<p class="caption">Análisis gráfico de la varianza explicada</p>
</div>
<p>De la misma forma, obtenemos un análisis gráfico de la varianza acumulada.</p>
<div class="figure">
<img src="figure/unnamed-chunk-8-1.png" alt="" />
<p class="caption">Análisis gráfico de varianza acumulada</p>
</div>
<p>A continuación, se seleccionaron el número de componentes principales óptimo para reducir la dimensión mediante variables observables. Mediante el método del codo, se ha podido analizar gráficamente y elegir las componentes principales.</p>
<div class="figure">
<img src="figure/unnamed-chunk-9-1.png" alt="" />
<p class="caption">Método del codo</p>
</div>
<p>En las siguientes gráficas podremos observar la representación conjunta de variables y observaciones que relaciona visualmente las posibles relaciones entre las observaciones, las contribuciones de los individuos a las varianzas de las componentes y el peso de las variables en cada componentes principal.</p>
<p>Variables y observaciones en la primera y segunda componente principal:</p>
<div class="figure">
<img src="figure/unnamed-chunk-10-1.png" alt="" />
<p class="caption">Contribuciones para la primera y segunda componente</p>
</div>
<p>Variables y observaciones en la primera y tercera componente principal:</p>
<div class="figure">
<img src="figure/unnamed-chunk-11-1.png" alt="" />
<p class="caption">Contribuciones para la primera y tercera componente</p>
</div>
<p>Variables y observaciones en la segunda y tercera componente principal:</p>
<div class="figure">
<img src="figure/unnamed-chunk-12-1.png" alt="" />
<p class="caption">Contribuciones para la segunda y tercera componente</p>
</div>
<h3 id="análisis-factorial">Análisis factorial</h3>
<p>Tras realizar el ACP, procedemos a realizar el Análisis Factorial (AF), el cual tiene sentido porque las variables están correladas:</p>
<div class="figure">
<img src="figure/unnamed-chunk-13-1.png" alt="" />
<p class="caption">Correlaciones</p>
</div>
<p>Una vez comparadas las salidas con el método del factor principal y con el de máxima verosimilitud, comparamos las comunalidades y las unicidades. De esta forma, ya podemos determinar el número óptimo de factores basándonos en los siguientes gráficos.</p>
<div class="figure">
<img src="figure/unnamed-chunk-14-1.png" alt="" />
<p class="caption">Scree plot</p>
</div>
<div class="figure">
<img src="figure/unnamed-chunk-15-1.png" alt="" />
<p class="caption">Análisis paralelo de scree plots</p>
</div>
<pre><code>## Parallel analysis suggests that the number of factors =  2  and the number of components =  NA</code></pre>
<p>Estimamos el modelo factorial con 3 factores implementando una rotación tipo varimax para buscar una interpretación más simple.</p>
<div class="figure">
<img src="figure/unnamed-chunk-16-1.png" alt="" />
<p class="caption">Contribuciones</p>
</div>
<p>Finalmente, con el test de hipótesis contrastamos si el número de factores es suficiente, lo cual fue cierto.</p>
<h3 id="análisis-de-la-normalidad-multivariante">Análisis de la normalidad multivariante</h3>
<p>Previo al análisis cluster, se ha analizado la normalidad multivariante de los datos con los tests de Royston y de Henze-Zirkler. Sin embargo ninguno de los dos tests encuentran evidencias al 5% de significación de falta de normalidad multivariante.</p>
<p>Tras realizar el test de Royston, obtenemos la gráfica que se muestra a continuación.</p>
<div class="figure">
<img src="figure/unnamed-chunk-17-1.png" alt="" />
<p class="caption">Test de Royston</p>
</div>
<h2 id="clasificación">Clasificación</h2>
<p>Para finalizar, se han aplicado técnicas de clustering. Hemos llegado a la conclusión de que el número óptimo de clusters para este dataset es 4:</p>
<div class="figure">
<img src="figure/unnamed-chunk-18-1.png" alt="" />
<p class="caption">Número óptimo de clusters</p>
</div>
<p>K-Means produce las siguientes particiones:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>p3</span></code></pre></div>
<div class="figure">
<img src="figure/unnamed-chunk-19-1.png" alt="" />
<p class="caption">Clusters</p>
</div>
<p>Este modelo nos arroja las siguientes siluetas:</p>
<div class="figure">
<img src="figure/unnamed-chunk-20-1.png" alt="" />
<p class="caption">Siluetas de K-Means, k = 4</p>
</div>
<h1 id="discusión-y-conclusiones">Discusión y conclusiones</h1>
<p>El objetivo principal consistía en realizar un agrupamiento de los objetos formando clusters de objetos con un alto grado de homogeneidad interna y heterogeneidad entre clusters.</p>
<p>En primer lugar, se ha obtenido el coeficiente de simetría de cada variable en la sección <a href="#Análisis%20exploratorio%20univariante">4.1</a>. En este problema, hemos obtenido valores positivos o negativos. Un valor negativo significa que la distribución está sesgada a la derecha, mientras que un valor positivo indica que la distribución se encuentra sesgada hacia la izquierda.</p>
<p>Uno de los puntos más controvertidos de este trabajo es la decisión de eliminar outliers. Como podemos observar en el diagrama de cajas de la sección <a href="#Materiales">3.1</a>, se muestran tres outliers en las variables <code>x4</code> y <code>x5</code>. Sin embargo, se ha tomado la decisión de no eliminarlos, pues si no fuera así, se ha comprobado que las métricas de rendimiento bajaban.</p>
<p>Por otra parte, hemos observado que algunas de nuestras variables no se distribuyen con respecto a una normal. Esto queda evidenciado por la gráfica de la sección <a href="#Análisis%20exploratorio%20univariante">4.1</a>. En concreto, las variables x6 y x8 dan problemas. Realizando un test de shapiro para estas variables, se obtiene lo siguiente:</p>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  datos_normalizados[, &quot;x6&quot;]
## W = 0.84811, p-value = 0.02694</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  datos_normalizados[, &quot;x8&quot;]
## W = 0.86598, p-value = 0.04623</code></pre>
<p>Ambos obtienen un p-valor por debajo de 0.05. Es probable que supongan una fuente de imprecisiones a la hora de pasar a la distribución normal multivariante.</p>
<p>En cuanto al Análisis Factorial de la sección <a href="#Análisis%20factorial">4.2.2</a>, se ha tomado la decisión de estimar 3 factores apoyándose en las gráficas obtenidas. Sin embargo, el análisis paralelo sugería que se estimasen 2 factores, lo cual produciría <a href="https://v8doc.sas.com/sashtml/stat/chap26/sect21.htm">un caso de ultra-Heywood</a>.</p>
<p>Finalmente, hay otro matiz muy importante que debemos discutir. <strong>Nuestro dataset solo tiene 13 instancias</strong>. Hacer un análisis de calidad con tan pocos datos no es posible. En general, clustering necesita decenas de instancias como mínimo para producir resultados realistas.</p>
<p>Para ejemplificar esto, la gráfica de las siluetas. Se genera una silueta de 0.37, la cual es considerablemente baja. Mirando los clusters, podemos ver cómo en el cluster 2 existen 2 elementos, mientras que en el 3 hay 3. Con esto no podemos llegar a ninguna conclusión.</p>
<p>Con el fin de encontrar agrupaciones pertinentes y producir un análisis de calidad, sería necesario ampliar el número de empresas. De esta forma, se podrían observar patrones más relevantes.</p>
<h1 id="distribución-de-las-tareas-entre-las-personas-implicadas">Distribución de las tareas entre las personas implicadas</h1>
<p>Para realizar este trabajo, no se han distribuido las tareas, sino que ambos hemos hecho todas las tareas a la vez, supervisando el uno al otro y poniendo en común nuestro conocimiento.</p>
